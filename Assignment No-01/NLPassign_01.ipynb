{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVP6HTjStlXX",
        "outputId": "4e5cf92a-b3ea-439b-dd86-f5faaecc7299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK (run once)\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (\n",
        "    word_tokenize,\n",
        "    WhitespaceTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer\n",
        ")\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "kEJWJJE5tnII"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyLA4tVHtqwQ",
        "outputId": "c5e335e6-d383-4036-8e80-e47d5c7ccb27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is a powerful NLP library! I'm learning tokenization, stemming, and lemmatization. #AI #NLP\"\n"
      ],
      "metadata": {
        "id": "6Msj3CVGttSD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wt = WhitespaceTokenizer()\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(wt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYK-uFi6twos",
        "outputId": "5d05a472-fafa-438e-f2c9-ca57d3c33436"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'NLP', 'library!', \"I'm\", 'learning', 'tokenization,', 'stemming,', 'and', 'lemmatization.', '#AI', '#NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treebank = TreebankWordTokenizer()\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "print(treebank.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVTfraoIt0VV",
        "outputId": "4781997b-f564-4372-865a-d6ecb27d37b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'NLP', 'library', '!', 'I', \"'m\", 'learning', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization.', '#', 'AI', '#', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tweet_tokenizer.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOZZYYZNt2Vk",
        "outputId": "3de5f016-69d4-4601-cd41-b2da51383a9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'NLP', 'library', '!', \"I'm\", 'learning', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', '#AI', '#NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mwe_tokenizer = MWETokenizer([('natural', 'language'), ('machine', 'learning')])\n",
        "mwe_text = \"I study natural language processing and machine learning.\"\n",
        "print(\"\\nMWE Tokenization:\")\n",
        "print(mwe_tokenizer.tokenize(mwe_text.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR-x9a6ZuEt0",
        "outputId": "4a50bc73-e560-464e-adc9-394ec61deae3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MWE Tokenization:\n",
            "['I', 'study', 'natural_language', 'processing', 'and', 'machine', 'learning.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Force download\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxMuaNwQuf_A",
        "outputId": "90cd0c90-8749-4924-8c69-2ee838adea22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}